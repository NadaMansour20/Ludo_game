{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport gym\nfrom gym import spaces\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T00:51:59.270020Z","iopub.execute_input":"2025-04-06T00:51:59.270322Z","iopub.status.idle":"2025-04-06T00:52:02.718735Z","shell.execute_reply.started":"2025-04-06T00:51:59.270301Z","shell.execute_reply":"2025-04-06T00:52:02.717922Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class LudoEnv(gym.Env):\n    def __init__(self):\n        super(LudoEnv, self).__init__()\n        self.num_pieces = 4\n        self.track_length = 57\n        self.action_space = spaces.Discrete(self.num_pieces)\n        self.observation_space = spaces.Box(low=0, high=self.track_length, shape=(8,), dtype=np.int32)\n        self.reset()\n\n    def reset(self):\n        self.positions = {\n            1: np.zeros(self.num_pieces, dtype=np.int32),\n            2: np.zeros(self.num_pieces, dtype=np.int32)\n        }\n        self.done = False\n        self.current_player = 1\n        return self._get_obs()\n\n    def _get_obs(self):\n        current = self.positions[self.current_player]\n        opponent = self.positions[3 - self.current_player]\n        return np.concatenate([current, opponent])\n    \n    def step(self, action):\n        if self.done:\n            return self._get_obs(), 0, True, False, {}\n\n        dice = np.random.randint(1, 7)\n        reward = 0\n        player = self.current_player\n\n        if self.positions[player][action] < self.track_length:\n            self.positions[player][action] += dice\n            if self.positions[player][action] >= self.track_length:\n                self.positions[player][action] = self.track_length\n                reward = 50\n            else:\n                reward = 1\n\n        if np.all(self.positions[player] >= self.track_length):\n            self.done = True\n            reward = 100\n\n        self.current_player = 3 - self.current_player\n        return self._get_obs(), reward, self.done, False, {}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T00:52:09.465474Z","iopub.execute_input":"2025-04-06T00:52:09.465906Z","iopub.status.idle":"2025-04-06T00:52:09.473928Z","shell.execute_reply.started":"2025-04-06T00:52:09.465882Z","shell.execute_reply":"2025-04-06T00:52:09.472953Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class DQN(nn.Module):\n    # Initializes the neural network layers.\n    def __init__(self, state_size, action_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_size, 256) # First fully connected layer that maps state to 256 hidden units\n        self.fc2 = nn.Linear(256, 256) # Second fully connected layer that processes the 256 hidden units\n        self.fc3 = nn.Linear(256, action_size) # Output layer that maps to the Q-values of all possible actions\n\n    #  Defines the forward pass of the network.\n    def forward(self, state):\n        x = torch.relu(self.fc1(state)) # Apply ReLU activation to the output of the first layer\n        x = torch.relu(self.fc2(x)) # Apply ReLU activation to the output of the second layer\n        return self.fc3(x) # Output Q-values from the final layer ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T00:52:21.762583Z","iopub.execute_input":"2025-04-06T00:52:21.762849Z","iopub.status.idle":"2025-04-06T00:52:21.767691Z","shell.execute_reply.started":"2025-04-06T00:52:21.762828Z","shell.execute_reply":"2025-04-06T00:52:21.766642Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ReplayBuffer:\n    # Initialize the buffer with a given capacity\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)# Using deque to store experiences, with a maximum length (capacity)\n     # Add a new experience to the buffer (state, action, reward, next_state, done)\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done)) # Append the experience to the buffer\n\n    # Sample a batch of experiences from the buffer\n    def sample(self, batch_size):\n        transitions = random.sample(self.buffer, batch_size) # Randomly select a batch of experiences\n        return zip(*transitions) # Return the experiences separated into different components \n\n    # Return the current size of the buffer\n    def __len__(self):\n        return len(self.buffer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T00:53:25.263311Z","iopub.execute_input":"2025-04-06T00:53:25.263568Z","iopub.status.idle":"2025-04-06T00:53:25.267912Z","shell.execute_reply.started":"2025-04-06T00:53:25.263545Z","shell.execute_reply":"2025-04-06T00:53:25.267163Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}